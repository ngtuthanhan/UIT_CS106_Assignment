{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw1tylEHwoL6"
      },
      "source": [
        "# Thông tin \n",
        "1. MSSV: 20520079\n",
        "2. Họ và tên: Nguyễn Tư Thành Nhân\n",
        "3. Bài tập: Assignment 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f5Q8PzLw090"
      },
      "source": [
        "# Import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8P_laMcSQNk"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHPxnLLww_bW"
      },
      "source": [
        "# Một số hàm để chơi game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGopsD0IWpDO"
      },
      "outputs": [],
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l8BKi9TSqRe"
      },
      "outputs": [],
      "source": [
        "def play_multiple_times(env, q_table, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, q_table)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1A47g5WxLLH"
      },
      "source": [
        "# Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFsyfXH5Ssd6"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3xVez-WTeww"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    q_table = np.ones((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        for step in range(num_steps_per_episode):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajkLeuRNxdVR"
      },
      "source": [
        "# SARSA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2HIIDktxiD5"
      },
      "outputs": [],
      "source": [
        "def choose_action(Q, epsilon, n_actions, s):\n",
        "    if np.random.random() <= epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[s, :])\n",
        "\n",
        "def sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    q_table = np.ones((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        action = choose_action(q_table, epsilon, n_actions ,state)\n",
        "\n",
        "        for step in range(num_steps_per_episode):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_action = choose_action(q_table, epsilon, n_actions , next_state)\n",
        "            \n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * q_table[next_state, next_action])\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdxxTL0txtJf"
      },
      "source": [
        "# Thực nghiệm trên FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xifGZ8j-SWPT"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmauQUIQVOWr",
        "outputId": "46605765-9b65-4311-cb2d-555e3b563846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 67/1000\n",
            "Average number of steps: 12.17910447761194\n",
            "Sum of rewards: 968.0\n"
          ]
        }
      ],
      "source": [
        "# Q Learning\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm4CcsAzSx-f",
        "outputId": "d894e6dc-9813-4b69-9c41-8fd8e31e3f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 63/1000\n",
            "Average number of steps: 10.746031746031745\n",
            "Sum of rewards: 800.0\n"
          ]
        }
      ],
      "source": [
        "# SARSA\n",
        "sarsa_table, rewards_all = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, sarsa_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYFY-7MdzKQ-"
      },
      "source": [
        "# Thực nghiệm trên FrozenLake8x8-v0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izqCqbPlzPLO"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake8x8-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpdoIhdezSCa",
        "outputId": "1afc50b0-9d77-4677-f662-7706341ac491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 2/1000\n",
            "Average number of steps: 33.0\n",
            "Sum of rewards: 75.0\n"
          ]
        }
      ],
      "source": [
        "# Q Learning\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1JzgYOqzTed",
        "outputId": "0f65ff05-1661-40d2-b54d-a886cc54a60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 4/1000\n",
            "Average number of steps: 28.25\n",
            "Sum of rewards: 34.0\n"
          ]
        }
      ],
      "source": [
        "# SARSA\n",
        "sarsa_table, rewards_all = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, sarsa_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtR85ZDGzULN"
      },
      "source": [
        "# Thực nghiệm trên Taxi-v3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRbSdDg1zpGm"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdgYPIsbzZkg",
        "outputId": "13352125-1181-4454-e78e-95b731a7e492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 12.998\n",
            "Sum of rewards: 5066.0\n"
          ]
        }
      ],
      "source": [
        "# Q Learning\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4EI-klnzask",
        "outputId": "e2ffb0ef-c5e0-4b7b-e9ea-4d7b10a3abdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.069\n",
            "Sum of rewards: 3669.0\n"
          ]
        }
      ],
      "source": [
        "# SARSA\n",
        "sarsa_table, rewards_all = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, sarsa_table, 1000)\n",
        "print('Sum of rewards:',sum(rewards_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8qkixdnzbIL"
      },
      "source": [
        "# Nhận xét:\n",
        "- Vì map FrozenLake8x8-v0 sẽ không thành công một lần chơi được với q_table khởi tạo là ma trận O (ma trận toàn số 0) (khi đó q_table sẽ giữ nguyên là ma trận O đến sau 20000 vòng lặp). Nên em đã thay thế bằng khởi tạo là ma trận toàn số 1. Tuy vậy, kết quả của map FrozenLake-v0 sẽ giảm đi (số lượng chơi thành công  giảm từ khoảng 700 xuống còn khoảng 70 lần)\n",
        "- Số lần chơi thành công và số bước chơi trung bình qua 3 map của 2 thuật toán Q Learning và SARSA là tương đương nhau \n",
        "- Tổng điểm thưởng của Q Learning cho kết quả tốt hơn so với SARSA"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "20520079.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
